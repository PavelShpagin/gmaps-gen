Article

NaviLoc: Visual Global Localization and Refinement
for GNSS-Denied UAV Navigation
Pavel Shpagin
Academia Tech; pavel.shpagin@theacademia.tech

Abstract
Visual localization of Unmanned Aerial Vehicles (UAVs) using satellite imagery enables GNSS-free
navigation but faces a fundamental challenge: the extreme domain gap between aerial and satellite
views causes visual place recognition (VPR) to fail unpredictably along the trajectory. We identify
that a primary cause of this failure is heading-dependent feature ambiguity—standard CNN features
are not rotation invariant, causing matches to degrade when the UAV’s heading deviates from the
satellite’s canonical North orientation. We present NaviLoc, a three-stage localization pipeline that
addresses this through heading rectification: after coarse global alignment, we rotate query images to
a canonical orientation using VIO-derived headings before extracting features for local refinement.
Combined with overlapping sliding-window SE(2) optimization, NaviLoc achieves 20.38m Absolute
Trajectory Error (ATE) on a challenging UAV-to-satellite benchmark—a 31× improvement over VIO
drift and 17× over state-of-the-art VPR methods. Our approach requires no dataset-specific tuning
and runs in real-time using a lightweight MobileNet-V3 backbone.
Keywords: visual localization; UAV navigation; visual place recognition; GNSS-denied navigation;
satellite imagery matching; heading rectification; cross-domain matching

1. Introduction
Global localization is essential for autonomous UAV navigation in GNSS-denied environments
such as urban canyons, indoor spaces, and adversarial conditions. Visual-Inertial Odometry (VIO)
provides accurate relative pose estimation but accumulates unbounded drift over extended trajectories.
Visual Place Recognition (VPR) offers absolute position estimates by matching current observations
against geo-referenced satellite imagery, but the extreme domain gap between nadir-view aerial images
and oblique satellite tiles leads to noisy, often incorrect matches.
The core challenge lies in the heterogeneous reliability of cross-domain visual similarity. Our
empirical analysis reveals that VPR confidence varies dramatically along a trajectory: some regions
produce reliable matches while others exhibit severe ambiguity. Prior work has addressed this through
confidence-based filtering or adaptive optimization strategies, but these approaches require careful
threshold tuning that does not generalize across datasets.
We identify a fundamental cause of this heterogeneity: heading-dependent feature ambiguity.
Standard convolutional neural networks extract features that are not rotation invariant. When a
UAV’s heading deviates from the satellite imagery’s canonical North orientation, the extracted features
become increasingly dissimilar to their true matches, even when the spatial location is correct. This
effect is most pronounced in regions with repeated visual patterns (e.g., agricultural fields, suburban
grids) where rotation compounds the inherent ambiguity.
Our key insight is that heading rectification—rotating query images to align with the satellite’s
canonical orientation before feature extraction—dramatically improves match reliability across the
entire trajectory. This transforms the heterogeneous optimization landscape into a more uniformly
tractable problem, enabling simple sliding-window refinement to achieve state-of-the-art accuracy
without dataset-specific tuning.

2 of 10

We present NaviLoc, a three-stage pipeline:
1.
2.
3.

Global Alignment: Coarse rotation and translation via median-based Procrustes alignment of
VPR matches.
Heading Rectification: Rotate query images to canonical North orientation using VIO-derived
headings and the estimated global rotation.
Sliding Window Refinement: Local SE(2) optimization on overlapping windows using rectified
features.

On a challenging UAV-to-satellite benchmark, NaviLoc achieves 20.38m ATE—a 31× improvement over VIO and 17× over AnyLoc-GeM. Our approach is parameter-free in the sense that the only
hyperparameters (window size and overlap) have intuitive defaults that generalize without tuning.

2. Related Work
Visual Place Recognition. VPR has evolved from handcrafted descriptors [1] to learned representations. NetVLAD [2] introduced end-to-end learning for place recognition. Recent work leverages
foundation models: AnyLoc [3] uses DINOv2 features with VLAD or GeM aggregation for universal place recognition. However, these methods produce per-image descriptors without trajectory
constraints, leading to noisy localization when applied frame-by-frame.
Cross-Domain Matching. The aerial-to-satellite domain gap has been addressed through domain
adaptation [6], synthetic data augmentation, and specialized architectures. Our approach is orthogonal:
rather than learning domain-invariant features, we transform the query domain to better match the
reference through geometric rectification.
Trajectory-Based Localization. Pose graph optimization [5] fuses VIO and visual constraints
but assumes Gaussian error distributions that poorly model VPR outliers. Sequence-based methods
exploit temporal consistency but require careful handling of the heterogeneous reliability landscape.
Our sliding-window approach provides a middle ground: local optimization with implicit smoothness
from window overlap.
Rotation Invariance. Rotation-equivariant networks [7] learn features that transform predictably
under rotation. We take a simpler approach: explicitly rectify images to a canonical orientation,
allowing standard CNN features to be used directly.

3. Materials and Methods
3.1. Problem Formulation
Given a sequence of N aerial query images { Ii }iN=1 with VIO-derived relative poses V = {( xiv , yiv )},
and a geo-referenced satellite map represented as M reference tiles with features Fr and coordinates
G = {( xrj , yrj )} jM=1 , we seek to estimate the global GPS positions P = {( xi , yi )}iN=1 of each query frame.
3.2. Algorithm Overview
Algorithm 1 presents the complete NaviLoc pipeline. The algorithm proceeds in three stages:
global alignment to establish coarse positioning, heading rectification to improve feature consistency,
and sliding-window refinement to achieve precise localization. Detailed pseudocode for each subroutine (G LOBAL A LIGN, C OMPUTE H EADINGS, S ELECTA NCHORS, O PTIMIZE SE2, AVERAGE O VERLAPS) is
provided in Appendix A.

3 of 10

Algorithm 1 NaviLoc: Global Localization and Refinement
Require: Query images { Ii }, VIO V, Reference (Fr , G)
Require: Window size W, Overlap O, Anchors per window K
Ensure: Localized positions P
1: // Stage 1: Global Alignment
2: Fq ← E XTRACT ({ Ii })
3: P, θ g ← G LOBAL A LIGN (Fq , Fr , V, G )
4: // Stage 2: Heading Rectification
5: ψ ← C OMPUTE H EADINGS (V, θ g )
6: { Ii′ } ← R ECTIFY ({ Ii }, ψ )
7: F′q ← E XTRACT ({ Ii′ })
8: // Stage 3: Sliding Window Refinement
9: Ws ← W − O
10: for k = 0, Ws , 2Ws , . . . while k < N do
11:
W ← [k, min(k + W, N ))
12:
A, T ← S ELECTA NCHORS (P[W ], F′q [W ], Fr , G, K )
13:
P[W ] ← O PTIMIZE SE2 (P[W ], A, T, F′q [W ], Fr , G)
14: end for
15: P ← AVERAGE O VERLAPS (P )
16: return P

▷ Step size

3.3. Stage 1: Global Alignment
We first establish a coarse global alignment using VPR retrieval and robust Procrustes estimation.
Feature Extraction. We extract features using MobileNet-V3-Small [4] pretrained on ImageNet.
For each image, we apply global average pooling followed by L2 normalization to obtain a 576dimensional descriptor.
q
VPR Retrieval. For each query feature fi , we retrieve the nearest reference by inner product:
q
j∗ = arg max j ⟨fi , frj ⟩. The retrieved coordinate g j∗ serves as a noisy target for frame i.
Robust SE(2) Estimation. We estimate the global rotation θ and translation t that align VIO
positions to VPR targets. To handle outliers, we use median-based translation estimation:


t∗ = mediani g ji∗ − Rθ vi

(1)

where Rθ is the 2D rotation matrix. We optimize θ via coarse-to-fine search, maximizing the mean
similarity at aligned positions.
3.4. Stage 2: Heading Rectification
The key insight of NaviLoc is that CNN features are not rotation invariant. When the UAV’s
heading differs from the satellite’s North orientation, feature similarity degrades even at the correct
location.
Heading Computation. We compute per-frame headings from VIO motion direction:
ψilocal = arctan 2(yiv+1 − yiv , xiv+1 − xiv )

(2)

The global heading is obtained by adding the estimated global rotation:
ψi = ψilocal + θ g

(3)

Image Rectification. Each query image is rotated by −ψi degrees to align its “up” direction with
North:
Ii′ = R OTATE ( Ii , −ψi )

(4)

Feature Re-extraction. We extract features from rectified images { Ii′ }. These heading-rectified
features exhibit significantly higher similarity to their true matches across the entire trajectory.

4 of 10

3.5. Stage 3: Sliding Window Refinement
With rectified features, we apply local SE(2) optimization using overlapping sliding windows.
Window Processing. We divide the trajectory into windows of size W with step size Ws = W − O
where O is the overlap. For each window, we perform anchor selection and SE(2) optimization.
Anchor Selection. Within each window, we identify the top-K highest-confidence frames as
anchors (we use K = 3). Confidence is measured by the cosine similarity between the query feature
and its nearest reference tile. These anchors provide reliable constraints for optimization.
SE(2) Optimization. We find the rotation θw and translation tw that maximize mean similarity
across the window. The rotation is optimized via coarse-to-fine search within a local range (±0.2 rad),
and translation is computed using median residuals from anchor points to their targets.
Overlap Averaging. Frames that appear in multiple windows receive multiple position estimates.
We average these to obtain the final position, providing implicit smoothing that prevents discontinuities
at window boundaries.
3.6. Implementation Details
We use MobileNet-V3-Small for efficiency (real-time on embedded GPUs). Default hyperparameters are W = 30, O = 15, and K = 3, chosen to balance local accuracy with global consistency. These
values are not tuned per-dataset.
During the preparation of this manuscript, the author used AI-assisted tools for drafting and
editing text. The author has reviewed and edited the output and takes full responsibility for the content
of this publication.

4. Results
4.1. Dataset
We evaluate on a challenging UAV-to-satellite benchmark collected in rural Ukraine. The dataset
consists of aerial imagery captured from a consumer drone flying over agricultural and semi-urban
terrain, matched against satellite reference tiles.
Table 1. Dataset statistics. The trajectory covers over 2.3 km with significant heading variation across diverse
terrain.

Property

Value

Query frames
Trajectory length
Query spacing (avg)
Reference tiles
Tile spacing
Map coverage
Query resolution
Reference resolution

58
2,323 m
40.7 m
462 (21 × 22 grid)
40 m
800 m × 840 m
1920 × 1080 px
256 × 256 px

Table 1 summarizes the dataset statistics. The trajectory spans over 2.3 km with significant
heading variation, presenting a challenging test for cross-domain localization. Ground truth GPS
coordinates are available for evaluation.
4.2. Baselines
We compare against:
•
•
•

VIO (start-aligned): VIO trajectory aligned to the first ground truth position. Represents pure
odometry drift.
MobileNet VPR (Top-3): Per-frame VPR using MobileNet-V3 features with top-3 averaging.
AnyLoc-GeM: State-of-the-art VPR using DINOv2-ViT-L/14 with GeM pooling [3].

5 of 10

•
•

AnyLoc-VLAD: DINOv2 with VLAD aggregation.
GlobalAlign: Our global alignment stage alone (no rectification or refinement).

4.3. Quantitative Results
Table 2. Quantitative comparison. NaviLoc achieves 20.38m ATE, a 31× improvement over VIO and 17× over
AnyLoc-GeM.

Method

ATE (m)

vs VIO

VIO (start-aligned)
AnyLoc-VLAD (Top-3)
MobileNet VPR (Top-3)
AnyLoc-GeM (Top-3)

626.67
357.30
347.47
321.36

1.0×
1.8×
1.8×
2.0×

GlobalAlign (Stage 1 only)
NaviLoc w/o Rectification
NaviLoc (Full)

50.27
27.88
20.38

12.5×
22.5×
30.7×

Table 2 presents our main results. Key observations:
VPR alone is insufficient. All VPR baselines (MobileNet, AnyLoc-GeM, AnyLoc-VLAD) achieve
only 1.8–2.0× improvement over VIO, with ATE exceeding 300m. The cross-domain gap causes severe
match failures.
Global alignment is critical. GlobalAlign (our Stage 1) achieves 50.27m ATE—a 12.5×
improvement—by leveraging VIO structure and robust estimation. This demonstrates the value
of trajectory-level reasoning.
Heading rectification enables robust refinement. Without rectification, sliding-window refinement achieves 27.88m. With rectification, we reach 20.38m—a 27% improvement. This validates our
hypothesis that heading-dependent feature ambiguity is a primary failure mode.
4.4. Ablation Study
Table 3. Ablation study on window size and overlap. Overlap averaging provides crucial smoothing.

Configuration

ATE (m)

GlobalAlign only
+ Sliding Window (W=20, O=0)
+ Sliding Window (W=25, O=0)
+ Sliding Window (W=30, O=0)
+ Sliding Window (W=30, O=15)

50.27
29.58
24.20
29.92
20.38

No rectification (W=30, O=15)

27.88

Table 3 ablates window size and overlap:
Window size matters moderately. Optimal window size (W=25–30) balances local accuracy with
having enough frames for robust anchor selection.
Overlap is essential. Without overlap (O=0), performance degrades significantly. Overlap
averaging provides implicit smoothing that prevents discontinuities at window boundaries.
4.5. Per-Segment Analysis
To understand where errors occur, we divide the trajectory into thirds (Head, Middle, Tail):

6 of 10

Table 4. Per-segment ATE (m). NaviLoc improves all segments uniformly.

Method

Head

Middle

Tail

GlobalAlign
NaviLoc

71.2
22.4

32.1
15.7

47.5
23.1

Table 4 shows that NaviLoc improves all segments uniformly, unlike prior methods that often
trade off head accuracy for tail accuracy. This uniformity stems from heading rectification making
features reliable across the entire trajectory.

5. Discussion
Why does heading rectification work? Standard CNNs learn features from images with a
canonical “up” direction (typically the image top). When applied to rotated images, these features
become increasingly dissimilar to their training distribution. By rectifying query images to match the
satellite’s North-up orientation, we restore feature consistency.
Limitations. Our approach assumes VIO provides accurate relative headings. In practice, VIO
heading drift is typically much smaller than position drift, making this assumption reasonable. However, in scenarios with significant magnetometer interference or prolonged hovering, heading estimates
may degrade.
Generalization. The hyperparameters (W = 30, O = 15, K = 3) were not tuned on this dataset
and represent intuitive defaults: windows should be large enough to contain reliable anchors but small
enough to allow local correction. We expect these values to transfer to other UAV datasets without
modification.

6. Conclusions
We presented NaviLoc, a visual localization pipeline that achieves state-of-the-art accuracy on
cross-domain UAV-to-satellite matching through heading rectification. By rotating query images to
a canonical North orientation before feature extraction, we transform a heterogeneous optimization
landscape into a uniformly tractable problem. Combined with overlapping sliding-window SE(2)
refinement, NaviLoc achieves 20.38m ATE—a 31× improvement over VIO drift—without datasetspecific tuning. Our lightweight implementation runs in real-time, enabling practical deployment on
resource-constrained UAV platforms.
Author Contributions: Conceptualization, P.S.; methodology, P.S.; software, P.S.; validation, P.S.; formal analysis,
P.S.; investigation, P.S.; resources, P.S.; data curation, P.S.; writing—original draft preparation, P.S.; writing—review
and editing, P.S.; visualization, P.S. The author has read and agreed to the published version of the manuscript.
Funding: This research received no external funding.
Institutional Review Board Statement: Not applicable.
Informed Consent Statement: Not applicable.
Data Availability Statement: The data supporting the reported results are not publicly available due to privacy
and confidentiality constraints.
Acknowledgments: The author acknowledges the open-source community for providing the foundational tools
and frameworks used in this research.
Conflicts of Interest: The author declares no conflicts of interest.

Abbreviations
The following abbreviations are used in this manuscript:

7 of 10

UAV
VIO
VPR
GNSS
ATE
CNN
SE(2)

Unmanned Aerial Vehicle
Visual-Inertial Odometry
Visual Place Recognition
Global Navigation Satellite System
Absolute Trajectory Error
Convolutional Neural Network
Special Euclidean Group in 2D

Appendix A. Detailed Pseudocode
This appendix provides detailed pseudocode for all subroutines used in Algorithm 1.
Algorithm A1 GlobalAlign: Robust SE(2) Alignment via VPR
Require: Query features Fq , Reference features Fr , VIO positions V, Reference coords G
Require: Coarse search steps Nc = 36, Fine search range δ = 0.2 rad
Ensure: Aligned positions P, Global rotation θ g
1: // VPR Retrieval: Find nearest reference for each query
2: for i = 1 . . . N do
3:
ji∗ ← arg max j ⟨Fq [i ], Fr [ j]⟩
4:
ti ← G[ ji∗ ]
▷ VPR target
5: end for
6: // Coarse rotation search
7: functionS CORE(θ)

cos θ − sin θ
8:
R←
sin θ
cos θ
⊤
9:
Vrot ← R · V
▷ Rotate VIO
10:
t∗ ← mediani (ti − Vrot [i ])
▷ Robust translation
⊤ + t∗
11:
P ← Vrot
12:
// Compute similarity at aligned positions
13:
for i = 1 . . . N do
14:
j ← arg min j ∥P[i ] − G[ j]∥
▷ Nearest tile
15:
si ← ⟨Fq [i ], Fr [ j]⟩
16:
end for
17:
return ∑i si
18: end function
19: Θc ← {−π + 2πk
▷ Coarse grid
Nc : k = 0, . . . , Nc − 1}
20: θc∗ ← arg maxθ ∈Θc S CORE (θ )
21: // Fine rotation search (Brent’s method)
22: θ g ← B RENT M INIMIZE (− S CORE, [ θc∗ − δ, θc∗ + δ ])
23: // Apply
 final transformation

cos θ g − sin θ g
24: R g ←
sin θ g
cos θ g
25: Vrot ← R g · V⊤
26: t g ← mediani (ti − Vrot [i ])
⊤ +t
27: P ← Vrot
g
28: return P, θ g

8 of 10

Algorithm A2 ComputeHeadings: VIO-Based Heading Estimation
Require: VIO positions V = {( xiv , yiv )}iN=1 , Global rotation θ g
Ensure: Per-frame global headings ψ = {ψi }iN=1
1: // Compute local headings from motion direction
2: for i = 1 . . . N − 1 do
3:
∆x ← xiv+1 − xiv
4:
∆y ← yiv+1 − yiv
5:
ψilocal ← arctan 2(∆y, ∆x )
6: end for
7: ψlocal
← ψlocal
N
N −1
8: // Transform to global frame
9: for i = 1 . . . N do
10:
ψi ← ψilocal + θ g
11: end for
12: return ψ

▷ Repeat last heading

Algorithm A3 SelectAnchors: Confidence-Based Anchor Selection
Require: Window positions Pw , Window features Fw , Reference (Fr , G)
Require: Number of anchors K
Ensure: Anchor indices A, Target coordinates T
1: n ← |Pw |
2: // Compute confidence for each frame in window
3: for i = 1 . . . n do
4:
ji ← arg min j ∥Pw [i ] − G[ j]∥
5:
ci ← ⟨Fw [i ], Fr [ ji ]⟩
6: end for
7: // Select top-K highest confidence as anchors
8: K ′ ← min( K, n )
9: A ← A RG T OP K ({ ci }in=1 , K ′ )
10: // Get target coordinates for anchors
11: T ← { G [ ji ] : i ∈ A}
12: return A, T

▷ Window size
▷ Nearest reference tile
▷ Cosine similarity
▷ Handle small windows

9 of 10

Algorithm A4 OptimizeSE2: Local Rotation and Translation Optimization
Require: Window positions Pw , Anchor indices A, Anchor targets T
Require: Window features Fw , Reference (Fr , G)
Require: Coarse steps Nc = 12, Search range θmax = 0.2 rad
Ensure: Refined positions P′w
1: functionS CORE(θ)

cos θ − sin θ
2:
R←
sin θ
cos θ
3:
Prot ← R · P⊤
w
4:
// Compute translation from anchor residuals
5:
t ← mediani∈A (T[i ] − Prot [i ])
⊤ +t
6:
P′ ← Prot
7:
// Evaluate similarity at transformed positions
8:
s←0
9:
for i = 1 . . . |Pw | do
10:
j ← arg min j ∥P′ [i ] − G[ j]∥
11:
s ← s + ⟨Fw [i ], Fr [ j]⟩
12:
end for
13:
return s
14: end function
15: // Coarse rotation search
2θmax k
: k = 0, . . . , Nc − 1}
16: Θc ← {−θmax + N
c
∗
17: θc ← arg maxθ ∈Θc S CORE (θ )
18: // Fine rotation search
19: θ ∗ ← B RENT M INIMIZE (− S CORE, [ θc∗ − 0.2, θc∗ + 0.2])
20: // Apply
transformation
 optimal

∗ − sin θ ∗
cos
θ
∗
21: R ←
sin θ ∗
cos θ ∗
22: Prot ← R∗ · P⊤
w
23: t∗ ← mediani ∈A (T [i ] − Prot [i ])
⊤ + t∗
24: P′w ← Prot
25: return P′w

▷ Rotate positions

Algorithm A5 AverageOverlaps: Overlap Consensus Averaging
Require: Position estimates from all windows (stored during refinement)
Require: Window size W, Overlap O, Trajectory length N
Ensure: Final positions P
1: S ← 0 N ×2
▷ Sum of position estimates
2: C ← 0 N
▷ Count of estimates per frame
3: Ws ← W − O
▷ Step size
4: // Accumulate estimates from each window
5: for k = 0, Ws , 2Ws , . . . while k < N do
6:
W ← [k, min(k + W, N ))
7:
Pw ← refined positions for window W
8:
for i ∈ W do
9:
S [i ] ← S [i ] + Pw [i − k ]
10:
C [i ] ← C [i ] + 1
11:
end for
12: end for
13: // Compute average
14: for i = 1 . . . N do
15:
P[i ] ← S[i ]/C[i ]
16: end for
17: return P

10 of 10

References
1.
2.

3.
4.

5.

6.

7.

Lowe, D.G. Distinctive Image Features from Scale-Invariant Keypoints. Int. J. Comput. Vis. 2004, 60, 91–110.
Arandjelović, R.; Gronat, P.; Torii, A.; Pajdla, T.; Sivic, J. NetVLAD: CNN Architecture for Weakly Supervised
Place Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), Las Vegas, NV, USA, 27–30 June 2016; pp. 5297–5307.
Keetha, N.; Mishra, A.; Karhade, J.; Jatavallabhula, K.M.; Scherer, S.; Krishna, M.; Garg, S. AnyLoc: Towards
Universal Visual Place Recognition. IEEE Robot. Autom. Lett. 2023, 8, 3082–3089.
Howard, A.; Sandler, M.; Chu, G.; Chen, L.-C.; Chen, B.; Tan, M.; Wang, W.; Zhu, Y.; Pang, R.; Vasudevan, V.;
Le, Q.V.; Adam, H. Searching for MobileNetV3. In Proceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV), Seoul, Republic of Korea, 27 October–2 November 2019; pp. 1314–1324.
Cadena, C.; Carlone, L.; Carrillo, H.; Latif, Y.; Scaramuzza, D.; Neira, J.; Reid, I.; Leonard, J.J. Past, Present,
and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age. IEEE Trans.
Robot. 2016, 32, 1309–1332.
Workman, S.; Souvenir, R.; Jacobs, N. Wide-Area Image Geolocalization with Aerial Reference Imagery.
In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Santiago, Chile, 7–13
December 2015; pp. 3961–3969.
Cohen, T.; Welling, M. Group Equivariant Convolutional Networks. In Proceedings of the 33rd International
Conference on Machine Learning (ICML), New York, NY, USA, 19–24 June 2016; pp. 2990–2999.

Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those
of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s)
disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or
products referred to in the content.

